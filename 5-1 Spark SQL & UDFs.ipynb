{"cells":[{"cell_type":"markdown","source":["d\n## Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources\nThis notebook contains for code samples for *Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources*."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d94882a8-37b6-4d1c-9c36-1a1e1c2d63e7"}}},{"cell_type":"markdown","source":["### User Defined Functions\nWhile Apache Spark has a plethora of functions, the flexibility of Spark allows for data engineers and data scientists to define their own functions (i.e., user-defined functions or UDFs)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a71c194-2c8f-492a-933a-449dc22b16ef"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\n# Create cubed function\ndef cubed(s):\n  return s * s * s\n\n# Register UDF\nspark.udf.register(\"cubed\", cubed, LongType())\n\n# Generate temporary view\nspark.range(1, 9).createOrReplaceTempView(\"udf_test\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8abac084-292d-4713-bab5-73e87674b906"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"605947c6-6ea7-420a-aab0-2b5a6b901ec1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Speeding up and Distributing PySpark UDFs with Pandas UDFs\nOne of the previous prevailing issues with using PySpark UDFs was that it had slower performance than Scala UDFs.  This was because the PySpark UDFs required data movement between the JVM and Python working which was quite expensive.   To resolve this problem, pandas UDFs (also known as vectorized UDFs) were introduced as part of Apache Spark 2.3. It is a UDF that uses Apache Arrow to transfer data and utilizes pandas to work with the data. You simplify define a pandas UDF using the keyword pandas_udf as the decorator or to wrap the function itself.   Once the data is in Apache Arrow format, there is no longer the need to serialize/pickle the data as it is already in a format consumable by the Python process.  Instead of operating on individual inputs row-by-row, you are operating on a pandas series or dataframe (i.e. vectorized execution)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9513ca7c-aa22-47d3-ad31-b96c7b6cf8cf"}}},{"cell_type":"code","source":["%python\nimport pandas as pd\n# Import various pyspark SQL functions including pandas_udf\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql.types import LongType\n\n# Declare the cubed function \ndef cubed(a: pd.Series) -> pd.Series:\n    return a * a * a\n\n# Create the pandas UDF for the cubed function \ncubed_udf = pandas_udf(cubed, returnType=LongType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b2491c4-31a0-4fc2-b757-7e8a4d5a1100"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Using pandas dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61eb36e7-c32c-457b-b902-c52bf8e6a0cf"}}},{"cell_type":"code","source":["%python\n# Create a Pandas series\nx = pd.Series([1, 2, 3])\n\n# The function for a pandas_udf executed with local Pandas data\nprint(cubed(x))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"497c7da2-fb79-4925-a21f-fea13579a654"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Using Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d99aa0fb-3b42-4e5b-a1bd-89e524682737"}}},{"cell_type":"code","source":["%python\n# Create a Spark DataFrame\ndf = spark.range(1, 4)\n\n# Execute function as a Spark vectorized UDF\ndf.select(\"id\", cubed_udf(col(\"id\"))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de7ca7e0-1224-47ba-a777-f0f0b8359620"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Higher Order Functions in DataFrames and Spark SQL\n\nBecause complex data types are an amalgamation of simple data types, it is tempting to manipulate complex data types directly. As noted in the post *Introducing New Built-in and Higher-Order Functions for Complex Data Types in Apache Spark 2.4* there are typically two solutions for the manipulation of complex data types.\n1. Exploding the nested structure into individual rows, applying some function, and then re-creating the nested structure as noted in the code snippet below (see Option 1)  \n1. Building a User Defined Function (UDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7584badf-ac42-42af-b4fe-d5abb9192288"}}},{"cell_type":"code","source":["# Create an array dataset\narrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n\n# Create schema\nfrom pyspark.sql.types import *\narraySchema = (StructType([\n      StructField(\"id\", IntegerType(), True), \n      StructField(\"values\", ArrayType(IntegerType()), True)\n      ]))\n\n# Create DataFrame\ndf = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\ndf.createOrReplaceTempView(\"table\")\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2944da83-3786-4014-b2a6-aead1bebc2a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Option 1: Explode and Collect\nIn this nested SQL statement, we first `explode(values)` which creates a new row (with the id) for each element (`value`) within values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d45834ca-e900-4133-959d-14ec97cbba22"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT id, collect_list(value + 1) AS newValues\n  FROM  (SELECT id, explode(values) AS value\n        FROM table) x\n GROUP BY id\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15b67b4e-491a-4a84-ac32-0eb3d4fcd123"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Option 2: User Defined Function\nTo perform the same task (adding a value of 1 to each element in `values`), we can also create a user defined function (UDF) that uses map to iterate through each element (`value`) to perform the addition operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34fab5ae-9c75-4149-aa68-6b22986cc671"}}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import ArrayType\n\n# Create UDF\ndef addOne(values):\n  return [value + 1 for value in values]\n\n# Register UDF\nspark.udf.register(\"plusOneIntPy\", addOne, ArrayType(IntegerType()))  \n\n# Query data\nspark.sql(\"SELECT id, plusOneIntPy(values) AS values FROM table\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86c8e52c-90d4-4316-9d9c-819ee4b21783"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Higher-Order Functions\nIn addition to the previously noted built-in functions, there are high-order functions that take anonymous lambda functions as arguments."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c122aa76-17bd-4570-b98d-647f2575b225"}}},{"cell_type":"code","source":["# In Python\nfrom pyspark.sql.types import *\nschema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n\nt_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\nt_c = spark.createDataFrame(t_list, schema)\nt_c.createOrReplaceTempView(\"tC\")\n\n# Show the DataFrame\nt_c.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e6565c8-9f40-407f-8566-7487b5bb081e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Transform\n\n`transform(array<T>, function<T, U>): array<U>`\n\nThe transform function produces an array by applying a function to each element of an input array (similar to a map function)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0bb80db-3eb6-45f4-8672-0b5fb787c016"}}},{"cell_type":"code","source":["# Calculate Fahrenheit from Celsius for an array of temperatures\nspark.sql(\"\"\"SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit FROM tC\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff93cdd2-f5ab-4b1f-896c-e993cbb5e80a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Filter\n\n`filter(array<T>, function<T, Boolean>): array<T>`\n\nThe filter function produces an array where the boolean function is true."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"760940c5-1b37-40a2-8c93-d40cc90c9f65"}}},{"cell_type":"code","source":["# Filter temperatures > 38C for array of temperatures\nspark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"596a6360-8098-4a81-9918-650d2f6e3df8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Exists\n\n`exists(array<T>, function<T, V, Boolean>): Boolean`\n\nThe exists function returns true if the boolean function holds for any element in the input array."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3cb18bb-6b0f-4a86-b607-36ac81ee5eff"}}},{"cell_type":"code","source":["# Is there a temperature of 38C in the array of temperatures\nspark.sql(\"\"\"\nSELECT celsius, exists(celsius, t -> t = 38) as threshold\nFROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f138b45-77c2-47ad-a414-fb3815b519c4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Reduce\n\n`reduce(array<T>, B, function<B, T, B>, function<B, R>)`\n\nThe reduce function reduces the elements of the array to a single value  by merging the elements into a buffer B using function<B, T, B> and by applying a finishing function<B, R> on the final buffer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d981fc2a-698d-4f26-8690-3f87714ffb54"}}},{"cell_type":"code","source":["# Calculate average temperature and convert to F\nspark.sql(\"\"\"\nSELECT celsius, \n       reduce(\n          celsius, \n          0, \n          (t, acc) -> t + acc, \n          acc -> (acc div size(celsius) * 9 div 5) + 32\n        ) as avgFahrenheit \n  FROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82eec166-1d0f-4b7f-acc1-a7faf7fdfd77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrames and Spark SQL Common Relational Operators\n\nThe power of Spark SQL is that it contains many DataFrame Operations (also known as Untyped Dataset Operations). \n\nFor the full list, refer to [Spark SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html).\n\nIn the next section, we will focus on the following common relational operators:\n* Unions and Joins\n* Windowing\n* Modifications"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ef06ec2-7f86-49d8-be5f-cf5dca016e44"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\n# Set File Paths\ndelays_path = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\nairports_path = \"/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n\n# Obtain airports dataset\nairports = spark.read.options(header='true', inferSchema='true', sep='\\t').csv(airports_path)\nairports.createOrReplaceTempView(\"airports_na\")\n\n# Obtain departure Delays data\ndelays = spark.read.options(header='true').csv(delays_path)\ndelays = (delays\n          .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n          .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n\ndelays.createOrReplaceTempView(\"departureDelays\")\n\n# Create temporary small table\nfoo = delays.filter(expr(\"\"\"\n            origin == 'SEA' AND \n            destination == 'SFO' AND \n            date like '01010%' AND \n            delay > 0\"\"\"))\n\nfoo.createOrReplaceTempView(\"foo\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d01daad1-6755-4ad5-b58b-a6b1bdcb8337"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23030d91-f0f6-466b-ae3a-61ca6ec06623"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e0caa99-f0e0-4fdd-bcab-dedee8be4c68"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM foo LIMIT 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9ffa649-56c9-47a4-8b8f-35c9f406ee1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Unions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fde706fd-c039-4797-b017-d00bcfa8ea70"}}},{"cell_type":"code","source":["# Union two tables\nbar = delays.union(foo)\nbar.createOrReplaceTempView(\"bar\")\nbar.filter(expr(\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cf1aa56-3e9d-4472-a8aa-08e1c4b51b8d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT * \nFROM bar \nWHERE origin = 'SEA' \n   AND destination = 'SFO' \n   AND date LIKE '01010%' \n   AND delay > 0\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09981cdb-802b-4597-b8c9-2d3fa6c712bd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Joins\nBy default, it is an `inner join`.  There are also the options: `inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti`.\n\nMore info available at:\n* [PySpark Join](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=join)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6af40cb-5fe9-4ada-8a66-99f0f9eae309"}}},{"cell_type":"code","source":["# Join Departure Delays data (foo) with flight info\nfoo.join(\n  airports, \n  airports.IATA == foo.origin\n).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a9e0050-fc6f-4e0a-b08e-c29b9f030d63"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n  FROM foo f\n  JOIN airports_na a\n    ON a.IATA = f.origin\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47ce9c99-fd1a-40f8-acc9-3e1b2cd687cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Windowing Functions\n\nGreat reference: [Introduction Windowing Functions in Spark SQL](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\n\n> At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the Frame. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb7d5e48-c6f0-4b9c-96b7-5628d256c74b"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")\nspark.sql(\"\"\"\nCREATE TABLE departureDelaysWindow AS\nSELECT origin, destination, sum(delay) as TotalDelays \n  FROM departureDelays \n WHERE origin IN ('SEA', 'SFO', 'JFK') \n   AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \n GROUP BY origin, destination\n\"\"\")\n\nspark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6a71ec5-d014-45dd-9bd6-350be23faab6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What are the top three total delays destinations by origin city of SEA, SFO, and JFK?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8785d4df-6bde-4b17-8d17-9f03de84debf"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT origin, destination, sum(TotalDelays) as TotalDelays\n FROM departureDelaysWindow\nWHERE origin = 'SEA'\nGROUP BY origin, destination\nORDER BY sum(TotalDelays) DESC\nLIMIT 3\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff3a61f2-9949-4d91-b5ed-d9d62902c951"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT origin, destination, TotalDelays, rank \n  FROM ( \n     SELECT origin, destination, TotalDelays, dense_rank() \n       OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank \n       FROM departureDelaysWindow\n  ) t \n WHERE rank <= 3\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb9cec14-8fb2-4b34-bff5-e69f20b4177b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Modifications\n\nAnother common DataFrame operation is to perform modifications to the DataFrame. Recall that the underlying RDDs are immutable (i.e. they do not change) to ensure there is data lineage for Spark operations. Hence while DataFrames themselves are immutable, you can modify them through operations that create a new, different DataFrame with different columns, for example."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2246ab42-bd80-432f-b49a-cc3a560855a5"}}},{"cell_type":"markdown","source":["### Adding New Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"497a711b-9d8f-4722-9321-d3caa55e8c5e"}}},{"cell_type":"code","source":["foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))\nfoo2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"677cdbbc-086e-4565-a01e-5ba169fce69c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT *, CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END AS status FROM foo\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edb017ed-78fe-4d57-a52a-86934d02041c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Dropping Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74c855ae-f0d3-414c-a4a9-3709a50e675c"}}},{"cell_type":"code","source":["foo3 = foo2.drop(\"delay\")\nfoo3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0a22ed3-fe26-4c4f-aab2-5066db5141dc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Renaming Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e20f2751-60bf-4346-80f7-6d86bea0f059"}}},{"cell_type":"code","source":["foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\nfoo4.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37e94e76-b5e2-4f91-b9b8-0413414af069"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pivoting\nGreat reference [SQL Pivot: Converting Rows to Columns](https://databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18614995-8403-4c87-a563-ca7fe4f87fb2"}}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay FROM departureDelays WHERE origin = 'SEA'\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"512a1f1e-eb59-4a1b-bb70-abcc794466fe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT * FROM (\nSELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n  FROM departureDelays WHERE origin = 'SEA' \n) \nPIVOT (\n  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n  FOR month IN (1 JAN, 2 FEB, 3 MAR)\n)\nORDER BY destination\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad0b27da-f6b2-4625-985c-c0119ed85610"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT * FROM (\nSELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n  FROM departureDelays WHERE origin = 'SEA' \n) \nPIVOT (\n  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n  FOR month IN (1 JAN, 2 FEB)\n)\nORDER BY destination\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed0ed30c-89e4-4557-ba12-b37bcda93c87"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Rollup\nRefer to [What is the difference between cube, rollup and groupBy operators?](https://stackoverflow.com/questions/37975227/what-is-the-difference-between-cube-rollup-and-groupby-operators)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e509a9-214b-43ff-90d0-83d6be14b5ae"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5-1 Spark SQL & UDFs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1331216426051523}},"nbformat":4,"nbformat_minor":0}
