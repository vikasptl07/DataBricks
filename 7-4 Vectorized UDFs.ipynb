{"cells":[{"cell_type":"markdown","source":["d\n# Vectorized User Defined Functions\n\nLet's compare the performance of UDFs, Vectorized UDFs, and built-in methods."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e681c1a-52b5-43bd-ba9e-c682be7ced13"}}},{"cell_type":"markdown","source":["Start by generating some dummy data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df6c890a-28d4-4e07-a616-c658bf619626"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, count, rand, collect_list, explode, struct, count, pandas_udf\n\ndf = (spark\n      .range(0, 10 * 1000 * 1000)\n      .withColumn('id', (col('id') / 1000).cast('integer'))\n      .withColumn('v', rand()))\n\ndf.cache()\ndf.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281aa94a-e7a3-40f4-9851-99f30773219c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Incrementing a column by one\n\nLet's start off with a simple example of adding one to each value in our DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b2f22de-80c5-47da-9ee6-579c81d17b69"}}},{"cell_type":"markdown","source":["### PySpark UDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"276ec728-8288-4245-b5b8-de3e353f6825"}}},{"cell_type":"code","source":["%python\n@udf(\"double\")\ndef plus_one(v):\n    return v + 1\n\n%timeit -n1 -r1 df.withColumn('v', plus_one(df.v)).agg(count(col('v'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7e643d0-8a3b-47f0-ac93-725e765b46c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternate Syntax (can also use in the SQL namespace now)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e54b03-9b5e-40aa-952c-ca0621f4d496"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.types import DoubleType\n\ndef plus_one(v):\n    return v + 1\n  \nspark.udf.register(\"plus_one_udf\", plus_one, DoubleType())\n\n%timeit -n1 -r1 df.selectExpr(\"id\", \"plus_one_udf(v) as v\").agg(count(col('v'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f288597-068b-4964-badc-ae38400384a2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Scala UDF\n\nYikes! That took awhile to add 1 to each value. Let's see how long this takes with a Scala UDF."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5843f332-fa33-49f0-8246-ca95a5a358bb"}}},{"cell_type":"code","source":["%python\ndf.createOrReplaceTempView(\"df\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"107335cf-2041-47cd-919e-0b2958215bdb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nval df = spark.table(\"df\")\n\ndef plusOne: (Double => Double) = { v => v+1 }\nval plus_one = udf(plusOne)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20ea85ac-1254-4f56-9eda-e69a99e01f33"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\ndf.withColumn(\"v\", plus_one($\"v\"))\n  .agg(count(col(\"v\")))\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0920b9f-ed93-40d1-85d0-f7832cf30fd0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wow! That Scala UDF was a lot faster. However, as of Spark 2.3, there are Vectorized UDFs available in Python to help speed up the computation.\n\n* [Blog post](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)\n* [Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html#pyspark-usage-guide-for-pandas-with-apache-arrow)\n\n![Benchmark](https://databricks.com/wp-content/uploads/2017/10/image1-4.png)\n\nVectorized UDFs utilize Apache Arrow to speed up computation. Let's see how that helps improve our processing time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abe380c7-7bb7-48ad-a83a-df2034f2caab"}}},{"cell_type":"markdown","source":["[Apache Arrow](https://arrow.apache.org/), is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. See more [here](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n\nLet's take a look at how long it takes to convert a Spark DataFrame to Pandas with and without Apache Arrow enabled."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9802beae-2b48-43e1-b89a-8ed075e2a728"}}},{"cell_type":"code","source":["%python\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\n%timeit -n1 -r1 df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8208b4cc-935d-42de-9010-7cba775a6c43"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n\n%timeit -n1 -r1 df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e497377-edb7-4c49-9801-0cc637f7b2e0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Vectorized UDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab614801-202c-4372-8089-c240b20d249c"}}},{"cell_type":"code","source":["%python\n@pandas_udf('double')\ndef vectorized_plus_one(v):\n    return v + 1\n\n%timeit -n1 -r1 df.withColumn('v', vectorized_plus_one(df.v)).agg(count(col('v'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13e9c54a-0868-4ea2-8911-1d8e02c3e96e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alright! Still not as great as the Scala UDF, but at least it's better than the regular Python UDF!\n\nHere's some alternate syntax for the Pandas UDF."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c059db2f-4c64-4221-b62b-5c5a9127f68a"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import pandas_udf\n\ndef vectorized_plus_one(v):\n    return v + 1\n\nvectorized_plus_one_udf = pandas_udf(vectorized_plus_one, \"double\")\n\n%timeit -n1 -r1 df.withColumn('v', vectorized_plus_one_udf(df.v)).agg(count(col('v'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"131c23b2-8af8-462e-a3f9-0a67d06d7e8a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Built-in Method\n\nLet's compare the performance of the UDFs with using the built-in method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0c9e277-7dca-4258-b8d1-3087096bcc9e"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import lit\n\n%timeit -n1 -r1 df.withColumn('v', df.v + lit(1)).agg(count(col('v'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06375400-2aab-4b3b-928e-12927e2dbd6b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Computing subtract mean\n\nUp above, we were working with Scalar return types. Now we can use grouped UDFs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9be4dfe-10e8-416a-ba33-42a5b811832c"}}},{"cell_type":"markdown","source":["### PySpark UDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51f3cf73-2b3d-4662-8cfc-9bbe164b8c20"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql import Row\nimport pandas as pd\n\n@udf(ArrayType(df.schema))\ndef subtract_mean(rows):\n  vs = pd.Series([r.v for r in rows])\n  vs = vs - vs.mean()\n  return [Row(id=rows[i]['id'], v=float(vs[i])) for i in range(len(rows))]\n  \n%timeit -n1 -r1 (df.groupby('id').agg(collect_list(struct(df['id'], df['v'])).alias('rows')).withColumn('new_rows', subtract_mean(col('rows'))).withColumn('new_row', explode(col('new_rows'))).withColumn('id', col('new_row.id')).withColumn('v', col('new_row.v')).agg(count(col('v'))).show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5446d7c0-dda4-434c-a580-0ba6ef59dc3f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Vectorized UDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1662a4cc-3f4d-4cf2-98ca-1e2809f951a7"}}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import PandasUDFType\n\n@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\ndef vectorized_subtract_mean(pdf):\n\treturn pdf.assign(v=pdf.v - pdf.v.mean())\n\n%timeit -n1 -r1 df.groupby('id').apply(vectorized_subtract_mean).agg(count(col('v'))).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7d9f7a0-589b-4e5b-84b4-1a08fcaf7a80"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"7-4 Vectorized UDFs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1331216426051335}},"nbformat":4,"nbformat_minor":0}
